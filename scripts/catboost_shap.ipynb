{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CatBoost Event Category Classification & SHAP Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, the CatBoost classifier is used to predict the event categories bases on the feature values. The SHAP values are then calculated to understand the importance of the features in the prediction. The SHAP values are then used to explain the predictions of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structure of Input Data:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "\n",
    "# catboost\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# scikit-learn\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split, GroupShuffleSplit\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# shap\n",
    "import shap\n",
    "shap.initjs() # is interpreted by the Jupyter notebook to perform load the necessary JavaScript for SHAP's interactive visualizations to work in notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"../data/ground_truth_features_grouped_train.csv\")\n",
    "df_test = pd.read_csv(\"../data/ground_truth_features_grouped_test.csv\")\n",
    "\n",
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the head of the data\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Relevant Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify categorical columns\n",
    "categorical_columns = ['Temporal Status', 'Event Factuality', 'Keyword is Nsubj', 'Keyword is Dobj', 'Keyword is Pobj']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.dropna()\n",
    "df_test = df_test.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data for Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Select X and y**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train[['Measurability', 'Temporal Status','Event Factuality', 'Keyword is Nsubj', 'Keyword is Dobj', 'Keyword is Pobj']]\n",
    "X_test = df_test[['Measurability', 'Temporal Status','Event Factuality', 'Keyword is Nsubj', 'Keyword is Dobj', 'Keyword is Pobj']]\n",
    "y_train = df_train['category']\n",
    "y_test = df_test['category']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transform Target Variable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scikit needs 1 dim array for target variable\n",
    "y_train = y_train.values.ravel()\n",
    "y_test = y_test.values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the 'category' label encoder\n",
    "le_category = LabelEncoder()\n",
    "\n",
    "# Fit label encoder and return encoded labels\n",
    "y = le_category.fit_transform(y_train)\n",
    "\n",
    "# Create dictionary to map category to index\n",
    "mapping_category_to_index = {category: index for index, category in enumerate(le_category.classes_)}\n",
    "\n",
    "# Create a list of the category label names\n",
    "category_names = list(le_category.classes_)\n",
    "\n",
    "print(mapping_category_to_index, category_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transform Feature Variables**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This enables us to see the SHAP values for each subclass of the categorical feature. E.g. temporal_status.past etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform Numerical Measurability from String to Integer\n",
    "X_train['Measurability'] = X_train['Measurability'].astype('int')\n",
    "X_test['Measurability'] = X_test['Measurability'].astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get Mapping Features to Indices**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_feature_to_index = {feature: index for index, feature in enumerate(X_train.columns)}\n",
    "mapping_index_to_feature = {index: feature for index, feature in enumerate(X_train.columns)}\n",
    "mapping_feature_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify Categorical Columns for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define indices of categorical and numerical features\n",
    "idx_categorical_features = [X_train.columns.get_loc(col) for col in X_train.columns if col in categorical_columns]\n",
    "idx_numerical_features = [X_train.columns.get_loc(col) for col in X_train.columns if col not in categorical_columns]\n",
    "\n",
    "print(f'\\n{X_train.columns.to_list()} \\nidx of categorical features: {idx_categorical_features}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://forecastegy.com/posts/catboost-hyperparameter-tuning-guide-with-optuna/\n",
    "\n",
    "- Learning Rate: Imagine a choir where each singer adds their voice to create the perfect harmony. However, some singers have louder voices than others, so the choir director instructs them to adjust their volume to maintain balance. In CatBoost, the learning rate operates similarly—it scales the contribution of each decision tree to manage the overall balance and accuracy of the model. A smaller learning rate signifies that each tree offers a smaller “voice,” or a smaller update to the model, resulting in gradual learning. This can lead to higher accuracy but increases the risk of underfitting and longer training times.\n",
    "A larger learning rate, on the other hand, means each tree has a more significant impact on the model, speeding up the learning process. However, a high learning rate can result in overfitting or model instability. A range of 0.001 to 0.1 is a good starting point.\n",
    "\n",
    "- depth: You can think of the depth as the complexity or “height” of decision trees in your CatBoost model. A higher depth can capture more intricate patterns in your data, leading to better performanc But there’s a catch - the deeper the tree, the more time it takes to train, and the higher the risk of overfitting.\n",
    "\n",
    "- subsample: Subsampling is a technique used to randomly choose a fraction of the dataset when constructing each tree. This promotes diversity among the trees and helps reduce overfitting. The subsample parameter ranges I recommend go from 0.05 to 1. Lower values increase diversity but may result in underfitting.\n",
    "\n",
    "- min data in leaf: min_data_in_leaf specifies the minimum number of samples required to create a leaf, effectively controlling the split creation process. Think about it as: how many data points will the tree use to estimate a prediction? Higher values generate less complex trees, reducing overfitting risks, but might result in underfitting. Lower values lead to more complex trees that might overfit. I like to consider values between 1 and 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, f1_score\n",
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.1, log=True), # learning_rate is searched within a logarithmic scale from 1e-3 to 0.1\n",
    "        \"depth\": trial.suggest_int(\"depth\", 1, 10),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.1, 1.0),\n",
    "        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.05, 1.0),\n",
    "        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 1, 100),\n",
    "        # -------------\n",
    "        'leaf_estimation_method': 'Gradient',\n",
    "        'bootstrap_type': 'Bernoulli',\n",
    "        'objective': 'MultiClass',\n",
    "        'random_state': 42,\n",
    "        'verbose': 0,\n",
    "        \"eval_metric\" : 'TotalF1',\n",
    "        \"early_stopping_rounds\" : 100\n",
    "    }\n",
    "\n",
    "    model = CatBoostClassifier(**params)\n",
    "    model.fit(X_train, y_train, cat_features=idx_categorical_features)\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    # Return the F1-score to maximize\n",
    "    f1 = f1_score(y_test, predictions, average='macro')\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best hyperparameters:', study.best_params)\n",
    "print('Best weighted f1:', study.best_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CatBoost Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create CatBoost Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Options: \n",
    "* https://catboost.ai/en/docs/concepts/python-reference_catboostclassifier\n",
    "* https://catboost.ai/en/docs/references/training-parameters/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'leaf_estimation_method': 'Gradient',\n",
    "    'learning_rate': 0.05599447214373542,\n",
    "    'depth': 5,\n",
    "    'bootstrap_type': 'Bernoulli',\n",
    "    'objective': 'MultiClass',\n",
    "    'subsample': 0.14439138713112645,\n",
    "    'colsample_bylevel': 0.7334294913535412,\n",
    "    'min_data_in_leaf': 93,\n",
    "    'random_state': 42,\n",
    "    'verbose': 0,\n",
    "    \"eval_metric\" : 'TotalF1',\n",
    "    \"early_stopping_rounds\" : 100\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = CatBoostClassifier(**params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fit Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "model.fit(X_train, y_train, idx_categorical_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_names_capitalized = [name.capitalize() for name in category_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# report = classification_report(y_test, y_pred, target_names=category_names)\n",
    "report = classification_report(y_test, y_pred, target_names=category_names)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Convert confusion matrix to DataFrame for better label handling\n",
    "cm_df = pd.DataFrame(cm, index=category_names_capitalized, columns=category_names_capitalized)\n",
    "\n",
    "# Plotting\n",
    "sns.heatmap(cm_df, annot=True, fmt='g', cmap='Blues')\n",
    "\n",
    "# Plotting\n",
    "plt.ylabel('Actual Labels')\n",
    "plt.xlabel('Predicted Labels')\n",
    "\n",
    "# plt.savefig(f'plots/catboost_confusion_matrix.pdf', format='pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://catboost.ai/en/docs/concepts/fstr\n",
    "model.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine X_train and X_test\n",
    "X = pd.concat([X_train, X_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tree Explainer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_explainer = shap.TreeExplainer(model)\n",
    "\n",
    "shap_tree_values = tree_explainer.shap_values(X)\n",
    "\n",
    "type(shap_tree_values), len(shap_tree_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explainer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.Explainer(model)\n",
    "\n",
    "shap_values = explainer(X)\n",
    "\n",
    "type(shap_values), shap_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_names_cap = [name.capitalize() for name in category_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All Categories, All Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_tree_values, X, plot_type=\"bar\", class_names=category_names_cap) # Use shap_tree_values here\n",
    "# shap.summary_plot(shap_tree_values, X_train, plot_type=\"bar\", class_names=category_names) # Use shap_tree_values here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate mean SHAP values for each class\n",
    "mean_0 = np.mean(np.abs(shap_values.values[:, :, 0]), axis=0)\n",
    "mean_1 = np.mean(np.abs(shap_values.values[:, :, 1]), axis=0)\n",
    "mean_2 = np.mean(np.abs(shap_values.values[:, :, 2]), axis=0)\n",
    "mean_3 = np.mean(np.abs(shap_values.values[:, :, 3]), axis=0)\n",
    "\n",
    "name_0 = category_names[0]\n",
    "name_1 = category_names[1]\n",
    "name_2 = category_names[2]\n",
    "name_3 = category_names[3]\n",
    "\n",
    "df = pd.DataFrame({name_0: mean_0, name_1: mean_1, name_2: mean_2, name_3: mean_3})\n",
    "\n",
    "hls_colors = sns.color_palette('hls', 4)  # 4 for the number of categories\n",
    "# hls_colors = ['blue', 'purple', 'red', 'green'] # 4 for the number of categories\n",
    "\n",
    "# plot mean SHAP values\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "df.plot(kind='bar', ax=ax, color=hls_colors) \n",
    "\n",
    "ax.set_ylabel(\"Mean SHAP\", size=16)\n",
    "ax.set_xticklabels(X.columns, rotation=45, size=14)\n",
    "ax.legend(fontsize=14)\n",
    "\n",
    "# plt.savefig(f'plots/shap_all_vertical.eps', format='eps', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Specific Category and Specific Feature for further Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_name = 'Measurability'\n",
    "idx_feature = mapping_feature_to_index[feature_name]\n",
    "\n",
    "category_name = 'intention'\n",
    "idx_category = mapping_category_to_index[category_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specific Category and All Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Barplot for specific category\n",
    "shap.plots.bar(shap_values[:,:,idx_category], show=False)\n",
    "\n",
    "plt.title(f'SHAP values for \"{category_name}\"')\n",
    "\n",
    "# plt.savefig(f'plots/shap_all_features_{category_name}.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values = shap_values[:,:,idx_category], features = X, feature_names=X.columns.tolist(),  plot_type='bar', class_names=category_names) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specific Category and Subclasses of Specific Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"One way is to use a beeswarm plot for an individual feature. You can see what we mean in Figure 6. Here we have grouped the SHAP values for the odor feature base on the odor category. For example, you can see that a foul smell leads to higher SHAP values. These mushrooms are more likely to be poisonous. In the previous article, we used boxplots to get similar results.\"\n",
    "\n",
    "\"We won’t discuss the code for this plot in detail. In a nutshell, we need to create a new SHAP values object, shap_values_odor. This is done by “post-processing” the SHAP values so they are in the form we want. We replace the original SHAP values with the SHAP values for odor (line 24). We also replace the feature names with the odor categories (line 43). If we create shap_values_odor correctly, we can use the beeswarm function to create the plot (line 46).\"\n",
    "\n",
    "https://webcache.googleusercontent.com/search?q=cache:https://towardsdatascience.com/shap-for-categorical-features-with-catboost-8315e14dac1&sca_esv=ef71f34d62d470b0&strip=1&vwsrc=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def boxplot_shap_values(idx_feature, idx_category, feature_name, category_name):\n",
    "\n",
    "    #get shaply values and data\n",
    "    feature_values = shap_values[:,idx_feature, idx_category].values\n",
    "    feature_data = X[feature_name]\n",
    "\n",
    "    #split odor shap values based on odor category\n",
    "    feature_categories = feature_data.unique()\n",
    "\n",
    "    # try to sort the feature categories\n",
    "    feature_categories = sorted(feature_categories)\n",
    "\n",
    "    if feature_name == 'Event Factuality':\n",
    "        feature_categories = ['_', 'negative', 'low', 'medium', 'high', 'max']\n",
    "\n",
    "    feature_groups = []\n",
    "    for i in feature_categories:\n",
    "        relevant_values = feature_values[feature_data == i]\n",
    "        feature_groups.append(relevant_values)\n",
    "\n",
    "    labels = feature_categories\n",
    "\n",
    "    #plot boxplot\n",
    "    plt.figure(figsize=(8, 5))\n",
    "\n",
    "    plt.boxplot(feature_groups,labels=labels)\n",
    "\n",
    "    feature_name_pretty = feature_name.replace('_', ' ').capitalize()\n",
    "    category_name_pretty = category_name.capitalize()\n",
    "\n",
    "    plt.ylabel('SHAP values',size=16)\n",
    "    plt.xlabel(f'{feature_name_pretty}',size=16)\n",
    "    plt.title(f'SHAP Values for {feature_name_pretty} and {category_name_pretty}',size=18)\n",
    "\n",
    "    # plt.savefig(f'plots/shap_boxplot_{feature_name}_{category_name}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature_name in X.columns.to_list():\n",
    "    idx_feature = mapping_feature_to_index[feature_name]\n",
    "\n",
    "    for category_name in category_names:\n",
    "        idx_category = mapping_category_to_index[category_name]\n",
    "\n",
    "        boxplot_shap_values(idx_feature, idx_category, feature_name, category_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specific Cagegory, Specific Datapoint, and All Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"We then visualise the SHAP values of the first prediction using a waterfall plot (line 6). You can see this plot in Figure 2. This tells us how each of the categorical feature values has contributed to the prediction. For example, we can see that this mushroom has an almond (a) odor. This has decreased the log odds by 0.85. In other words, it has decreased the likelihood that the mushroom is poisonous.\" \n",
    "\n",
    "https://webcache.googleusercontent.com/search?q=cache:https://towardsdatascience.com/shap-for-categorical-features-with-catboost-8315e14dac1&sca_esv=ef71f34d62d470b0&strip=1&vwsrc=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Waterfall plot for specific observation, all features, and specific category\n",
    "\n",
    "category_name = 'intention'\n",
    "idx_category = mapping_category_to_index[category_name]\n",
    "\n",
    "# Chose a specific observation\n",
    "idx_observation = 33\n",
    "\n",
    "shap.plots.waterfall(shap_values[idx_observation,:,idx_category], show=False)\n",
    "plt.title(f'Waterfall Plot for observation {idx_observation} and category \"{category_name}\"')\n",
    "# plt.savefig(f'plots/shap_waterfall_plot_id{idx_observation}_{category_name}.png', dpi=100, bbox_inches='tight', facecolor='white')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simple_classification",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
